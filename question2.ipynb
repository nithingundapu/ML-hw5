{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzHZjxqG1oj-",
        "outputId": "8e2e4829-6a10-4c56-94f2-a572285b64b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([32, 10, 128])\n",
            "Attention shape: torch.Size([32, 8, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------------------------\n",
        "# Multi-Head Self Attention Layer\n",
        "# ------------------------------\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must divide evenly across heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        # Project inputs\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Split into heads: (B, num_heads, T, d_head)\n",
        "        def split_heads(t):\n",
        "            return t.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Q)\n",
        "        Kh = split_heads(K)\n",
        "        Vh = split_heads(V)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, Vh)\n",
        "\n",
        "        # Merge heads back\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "\n",
        "        return self.W_o(out), weights\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Transformer Encoder Block\n",
        "# ------------------------------\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model=128, num_heads=8, d_ff=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention + residual\n",
        "        attn_out, attn_weights = self.attn(x)\n",
        "        x = self.norm1(x + self.drop(attn_out))\n",
        "\n",
        "        # Feed-forward + residual\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.drop(ff_out))\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Test with Sample Input\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    batch = 32\n",
        "    seq_len = 10\n",
        "    d_model = 128\n",
        "\n",
        "    x = torch.randn(batch, seq_len, d_model)\n",
        "\n",
        "    encoder = TransformerEncoder(d_model=128, num_heads=8, d_ff=256)\n",
        "    out, attn = encoder(x)\n",
        "\n",
        "    print(\"Output shape:\", out.shape)\n",
        "    print(\"Attention shape:\", attn.shape)\n"
      ]
    }
  ]
}